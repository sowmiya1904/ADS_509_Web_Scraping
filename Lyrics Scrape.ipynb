{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95952cac",
   "metadata": {},
   "source": [
    "# ADS 509 Module 1: APIs and Web Scraping\n",
    "\n",
    "## Assignment 1.1 Data Acquisition\n",
    "\n",
    "This notebook has two parts. In the first part, you will scrape lyrics from AZLyrics.com. In the second part, you'll run code that verifies the completeness of your data pull. \n",
    "\n",
    "For this assignment you have chosen two musical artists who have at least 20 songs with lyrics on AZLyrics.com. We start with pulling some information and analyzing them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8969e",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185076b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c13af3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lyrics Scrape\n",
    "\n",
    "This section asks you to pull data by scraping www.AZLyrics.com. In the notebooks where you do that work you are asked to store the data in specific ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd7df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold the artist name and lyrics link\n",
    "artists = {'bigsean':\"https://www.azlyrics.com/b/bigsean.html\",\n",
    "           'johnlegend':\"https://www.azlyrics.com/j/johnlegend.html\"} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236c99b",
   "metadata": {},
   "source": [
    "## A Note on Rate Limiting\n",
    "\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to [a Tom Jones song](https://www.azlyrics.com/lyrics/tomjones/itsnotunusual.html).) \n",
    "\n",
    "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you _do_ get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again. \n",
    "\n",
    "## Part 1: Finding Links to Songs Lyrics\n",
    "\n",
    "That general artist page has a list of all songs for that artist with links to the individual song pages. \n",
    "\n",
    "**Q: Take a look at the `robots.txt` page on www.azlyrics.com. (You can read more about these pages [here](https://developers.google.com/search/docs/advanced/robots/intro).) Is the scraping we are about to do allowed or disallowed by this page? How do you know?**\n",
    "\n",
    "A: We can view the robots.txt file for the website by visiting https://www.azlyrics.com/robots.txt. This file outlines the rules that permit or restrict specific crawlers from accessing certain paths or directories. For our intended web scraping, it is allowed, as the file only disallows crawling of two particular directories and their contents, \"lyricsdb\" and \"song,\" while permitting access to all other URLs, except for one specific crawler (008).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9d31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up a dictionary of lists to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "for artist, artist_page in artists.items() :\n",
    "    # request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "    \n",
    "    # Extract the raw HTML content of the page\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    # Extracts all the 'a' tags with href attribute links\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href')\n",
    "        \n",
    "        # Check if the link matches the corresponding artist\n",
    "        if href.startswith(f'/lyrics/{artist}/'):\n",
    "            lyrics_pages[artist].append(href)               # Store the link to the dictionary where key is the artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285ec1",
   "metadata": {},
   "source": [
    "Let's make sure we have enough lyrics pages to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae4cda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items() :\n",
    "    assert(len(set(lp)) > 20)            # Checks if there are more than 20 lyrics links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edca10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For bigsean we have 247.\n",
      "The full pull will take for this artist will take 0.69 hours.\n",
      "For johnlegend we have 204.\n",
      "The full pull will take for this artist will take 0.57 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items(): \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011be6c6",
   "metadata": {},
   "source": [
    "## Part 2: Pulling Lyrics\n",
    "\n",
    "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part. \n",
    "\n",
    "1. Create an empty folder in our repo called \"lyrics\". \n",
    "1. Iterate over the artists in `lyrics_pages`. \n",
    "1. Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have `lyrics/cher/` in your repo.\n",
    "1. Iterate over the pages. \n",
    "1. Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
    "1. Use the function below, `generate_filename_from_url`, to create a filename based on the lyrics page, then write the lyrics to a text file with that name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67693711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate filename from url\n",
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https and the html\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "\n",
    "    name = name.replace(\"/lyrics/\",\"\")\n",
    "    \n",
    "    # Replace useless characters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15fb31a2-bca1-46dc-9bbc-f6191257b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title and lyrics \n",
    "url_stub = \"https://www.azlyrics.com\" \n",
    "\n",
    "def extract_lyrics(artist, link):\n",
    "    url = url_stub + link\n",
    "    response = requests.get(url)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "    \n",
    "    # Extracts the raw HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1').text.strip('\"').split('\"')[0]   # Extract title\n",
    "    lyrics = soup.find('div', class_=False, id=False).text  # Extract lyrics\n",
    "    return title, lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a78c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the lyrics folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then use shutil.rmtree to remove it and create a new one.\n",
    "\n",
    "if os.path.isdir(\"lyrics\") : \n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "\n",
    "os.mkdir(\"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d655b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Song Lyrics for bigsean...\n",
      "Extracting Song Lyrics for johnlegend...\n",
      "Extraction Completed...\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_pages = 0 \n",
    "\n",
    "for artist, links in lyrics_pages.items():\n",
    "    artist_links = links[:20]\n",
    "    \n",
    "    parent_dir = \"lyrics\"\n",
    "    # creating a subfolder for the artist\n",
    "    artist_dir = os.path.join(parent_dir, artist)\n",
    "    if os.path.isdir(artist_dir):\n",
    "        shutil.rmtree(artist_dir)\n",
    "    os.mkdir(artist_dir)\n",
    "    \n",
    "    print(f\"Extracting Song Lyrics for {artist}...\")\n",
    "    \n",
    "    for link in artist_links:\n",
    "        title, lyrics = extract_lyrics(artist, link)\n",
    "        filename = generate_filename_from_link(link)\n",
    "        file_path = os.path.join(artist_dir, filename)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(title)\n",
    "            f.write('\\n\\n')\n",
    "            f.write(lyrics)\n",
    "            \n",
    "print(\"Extraction Completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c394f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 0.15 hours.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cf14b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "This assignment asks you to pull data by scraping www.AZLyrics.com.  After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217c2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37778a1c",
   "metadata": {},
   "source": [
    "## Checking Lyrics \n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory:\n",
    "`/lyrics/[Artist Name]/[filename from URL]`. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bccac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For bigsean we have 20 files.\n",
      "For bigsean we have roughly 10837 words, 1731 are unique.\n",
      "For johnlegend we have 20 files.\n",
      "For johnlegend we have roughly 7868 words, 898 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc339c-5a21-459b-aa4b-e13e40adf7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
